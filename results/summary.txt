After reading this article, you will have a basic understanding of BERT and will be able to utilize it for your own business applications. It would be helpful if you are familiar with Python and have a general idea of machine learning. The BERT models I will cover in this article are: Binary or multi-class classification Regression model Question-answering applications Introduction to BERT BERT is trained on the entirety of Wikipedia (~2.5 billion words), along with a book corpus (~800 million words). In order to utilize BERT, you won’t have to repeat this compute-intensive process. BERT, Bi-directional Encoder Representation from Transformer, is a state of the art language model by Google which can be used for cutting-edge natural language processing (NLP) tasks. Transfer Learning Transfer learning is a process where a machine learning model developed for a general task can be reused as a starting point for a specific business problem. Imagine you want to teach someone named Amanda, who doesn’t speak English, how to take the SAT. The first step would be to teach Amanda the English language as thoroughly as possible. Then, you can teach her more specifically for the SAT. The first part of transfer learning is pre-training (similar to teaching Amanda English for the first time). BERT Pre-training This is a quick introduction about the BERT pre-training process. BERT takes two chunks of text as input. For practical purposes, you can use a pre-trained BERT model and do not need to perform this step. Sentence 1 starts with a special token In the pre-training for BERT, Sentence 2 intentionally does not follow Sentence 1 in about half of the training examples. There will be a single token for each word that is in the BERT vocabulary. In the simplified example above, I referred to these two inputs as Sentence 1 and Sentence 2. and both sentences end with another special token .
